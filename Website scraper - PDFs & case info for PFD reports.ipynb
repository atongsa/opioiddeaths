{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping the Courts and Tribunals Judiciary Website for Prevent Future Death Reports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from requests import ConnectionError\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from time import sleep\n",
    "from time import time\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "    \n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "    \n",
    "def get_url(url):\n",
    "    response = get(url, verify = False)\n",
    "    html = response.content\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    return soup\n",
    "\n",
    "def retries(record_url, tries=3):\n",
    "    for i in range(tries):\n",
    "        try:\n",
    "            soup = get_url(record_url)\n",
    "            return soup\n",
    "        except (ConnectionError, SSLError):\n",
    "            if i < tries - 1:\n",
    "                sleep(2)\n",
    "                continue\n",
    "            else:\n",
    "                return 'Con error'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraper starts here - this was run on Wednesday, Aug 12 2020 at 11.00 am."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your second value in \"range\" will be one more than the number of pages that exist on the wesbite\n",
    "pages = list(range(1,326))\n",
    "\n",
    "#This loops through all the pages to get the URLs to individual records\n",
    "page_string = 'https://www.judiciary.uk/subject/prevention-of-future-deaths/page/{}/'\n",
    "record_urls = []\n",
    "for page in tqdm(pages[0:1]):\n",
    "    soup = get_url(page_string.format(str(page)))\n",
    "    h5s = soup.find_all('h5', {'class': 'entry-title'})\n",
    "    for h5 in h5s:\n",
    "        record_urls.append(h5.a.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we check how many records (i.e. cases) were pulled from the urls & the first and last case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(record_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_urls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_urls[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is my second loop. This will go through the lists of URLs I just created above to visit each individual record and pull out and store the text data (info on the decreased/case) and the PDF URL I will use later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_details(e_dict, record_count, record_url, details):\n",
    "    e_dict['index'] = record_count\n",
    "    e_dict['url'] = record_url\n",
    "    e_dict['reason'] = details\n",
    "    return e_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_exp = re.compile(r\"â€™s\\s|s\\s|'s\\s\")\n",
    "text_cats = ['Date of report', 'Ref', 'Deceased name', 'Coroner name', 'Coroner Area', 'Category', \"This report is being sent to\"]\n",
    "#First, I create two lists, one for the PDFs and one for the text data\n",
    "record_text = []\n",
    "pdf_urls = []\n",
    "ref_list = []\n",
    "#I want to loop through each URL & pull out the death information and pdf link for downloading\n",
    "error_catching = []\n",
    "\n",
    "record_count = 0\n",
    "for record_url in tqdm(record_urls):\n",
    "    try:\n",
    "        error_dict = {}\n",
    "        #Calling the retries function\n",
    "        soup = retries(record_url, tries=5)\n",
    "        \n",
    "        if soup == 'Con error':\n",
    "            print(f\"{record_url} could not connect\")\n",
    "            error_catching.append(error_details(error_dict, record_count, record_url, 'Connection Error'))\n",
    "            record_count +=1\n",
    "            continue\n",
    "\n",
    "        #This gets all the text fields from the website to work with\n",
    "        death_info = soup.find('div', {'class':'entry-content'}).find_all('p')\n",
    "        \n",
    "        if not death_info:\n",
    "            print(f\"{record_url} produced no data\")\n",
    "            error_catching.append(error_details(error_dict, record_count, record_url, 'No Text Loaded'))\n",
    "            record_count +=1\n",
    "            continue\n",
    "            \n",
    "        #Our dictionary that will hold all of the text information that we will eventually append to \"record_text\"\n",
    "        blankdict = {}\n",
    "        \n",
    "        #This is to handle 1 annoying record with messed up html tags\n",
    "        if record_url == 'https://www.judiciary.uk/publications/roadsafety/':\n",
    "            strong = death_info[0].find_all('strong')\n",
    "            heads = ['date_of_report', 'ref', 'deceased_name', 'coroner_name', 'coroner_area', 'category']\n",
    "            for st, h in zip(strong,heads):\n",
    "                blankdict[h] = st.next_sibling.replace(':','').replace('Ref','').strip()\n",
    "        #And another record with wonky html\n",
    "        elif record_url == 'https://www.judiciary.uk/publications/helen-sheath/':\n",
    "            brs = death_info[0].text.split('\\n')\n",
    "            vals = []\n",
    "            for b in brs:\n",
    "                vals.append(b.split(':'))\n",
    "            for v in vals:\n",
    "                if v[0] == \"Coroners name\":\n",
    "                    alt = \"coroner_name\"\n",
    "                    blankdict[alt] = v[1].strip().replace('\\n','')\n",
    "                elif v[0] == \"Coroners Area\":\n",
    "                    alt = \"coroner_area\"\n",
    "                    blankdict[alt] = v[1].strip().replace('\\n','')\n",
    "                else:\n",
    "                    blankdict[v[0].strip().replace(' ','_').lower()] = v[1].strip().replace('\\n','')\n",
    "        else:        \n",
    "            #looping through all of the text categories for handling\n",
    "            for p in death_info:\n",
    "                #This checks for blank fields and if there is nothing, it skips it\n",
    "                if p.text.strip() == '':\n",
    "                    pass\n",
    "                #This checks for our \"Normal\" case in which a colon exists and the category is one of the ones we \n",
    "                #pre-specified above in the \"text_cats\" list\n",
    "                #We also need to account here for one strange record for \"Rebecca Evans\" which has a weird text error\n",
    "                #That we manually correct for\n",
    "                elif ':' in p.text and p.text.split(':')[0] in text_cats and not 'Rebecca-EvansR.pdf' in p.text:\n",
    "                    #Simply assigning the key and value from strings on either side of the colon, making everything \n",
    "                    #lower case and replacing spaces with underscores and also removing any stray semi-colons\n",
    "                    text_list = p.text.split(':')\n",
    "                    blankdict[text_list[0].strip().replace(' ','_').lower()] = text_list[1].strip().replace('\\n','').replace('\\xa0','')\n",
    "\n",
    "                elif 'Rebecca-EvansR.pdf' in p.text:\n",
    "                    #This deals with that singular odd record that currently exists as of 8 Nov 2019\n",
    "                    blankdict['category'] = p.text.split(':')[1].strip().replace('\\n','')\n",
    "                    \n",
    "                elif ':' not in p.text:\n",
    "                    #If the string doesn't have a colon, we can't split on it so have to get it into dictionary format\n",
    "                    #Using an alternate method that counts the length of the thing\n",
    "                    if any(x in p.text for x in text_cats):\n",
    "                        t = [x for x in text_cats if x in p.text][0]\n",
    "                        l = len(t)\n",
    "                        blankdict[t.replace(' ','_').lower()] = p.text[l+1:].replace('\\n','').replace('\\xa0','')\n",
    "                    elif 'Coroners Area' in p.text:\n",
    "                        blankdict['coroner_area'] = p.text[13:].strip().replace('\\n','').replace('\\xa0','')\n",
    "                    else:\n",
    "                        print(\"Something we haven't accounted for has happened\")\n",
    "\n",
    "                elif p.text.strip().count(\":\") == 2:\n",
    "                    #This corrects for one odd record in which there are 2 colons but should generalize to fix it for\n",
    "                    #any time this could happen, so long as it happens in the same way\n",
    "                    text_list = p.text.split(':')\n",
    "                    new_string = text_list[0] + text_list[1]\n",
    "                    new_name = re.sub(reg_exp, ' ', new_string).strip()\n",
    "                    blankdict[new_name.replace(' ','_').lower()] = text_list[2].strip().replace('\\n','').replace('\\xa0','')\n",
    "\n",
    "                elif ':' in p.text and p.text.split(':')[0] not in text_cats:\n",
    "                    #Some field names are in the form of \"name_of_decesased\" or \"name_of_coroner\" or are plural/\n",
    "                    #possessive so this smashes those into our preferred naming formats\n",
    "                    if 'Name of' in p.text:\n",
    "                        all_text = p.text.split(':')\n",
    "                        key_name = all_text[0].split(' ')\n",
    "                        blankdict[key_name[2].strip() + '_name'] = all_text[-1].strip()\n",
    "                    else:    \n",
    "                        new_name = re.sub(reg_exp, ' ', p.text)\n",
    "                        text_list = new_name.split(':')\n",
    "                        blankdict[text_list[0].strip().replace(' ','_').lower()] = text_list[1].strip().replace('\\n','').replace('\\xa0','')\n",
    "        blankdict['url'] = record_url\n",
    "        \n",
    "        #A small little check for duplicated ref names\n",
    "        try:\n",
    "            if not blankdict['ref']:\n",
    "                pass\n",
    "            elif blankdict['ref'] in ref_list:\n",
    "                blankdict['ref'] = blankdict['ref'] + 'A'\n",
    "            ref_list.append(blankdict['ref'])\n",
    "        except KeyError:\n",
    "            blankdict['ref'] = ''\n",
    "            \n",
    "        #This appends the final dict to the list\n",
    "        record_text.append(blankdict)\n",
    "        \n",
    "        #this is a seperate process to get the PDF URLs (no matter how many there are) and adds them to their own list   \n",
    "        urls = soup.find_all('li', {'class':'pdf'})\n",
    "        pdf_list = []\n",
    "        for url in urls:\n",
    "            pdf_list.append(url.findNext('a').get('href'))\n",
    "        pdf_urls.append(pdf_list)\n",
    "        \n",
    "        record_count += 1\n",
    "        \n",
    "    except Exception as e:\n",
    "        import sys\n",
    "        error_desc = f\"{str(e)} occurred for {record_url} when trying to work with {p}\"\n",
    "        print(error_desc)\n",
    "        error_catching.append(error_details(error_dict, record_count, record_url, error_desc))\n",
    "        \n",
    "        #Saving this in case we don't like the error catching.\n",
    "        #import sys\n",
    "        #raise type(e)(str(e) + '\\n' + 'Error for Record: {}, Field: {}'.format(record_url, p)).with_traceback(sys.exc_info()[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the third loop to save the PDFs using the deceased Ref as the file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Any errors should print out above, but you can also check the error_catching dict\n",
    "#Here we just turn it into a dataframe quickly to easily view\n",
    "\n",
    "error_df = pd.DataFrame(error_catching)\n",
    "error_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(path_string, name_string):\n",
    "    with open(path_string.format(name_string), 'wb') as d:\n",
    "        d.write(myfile.content)\n",
    "\n",
    "#save_path = '/Users/georgiarichards/Desktop/Python/PFDs opioids/All_PDFs5/{}.pdf'\n",
    "save_path = '/Users/nicholasdevito/Desktop/untitled folder/{}.pdf'\n",
    "\n",
    "potential_names = ['ref', 'deceased_name', 'date_of_report']\n",
    "\n",
    "record_count = 0\n",
    "#This is the final scrape to actually get the URLs and change the name (when possible) to the refs\n",
    "for r_t, p_u in zip(tqdm(record_text), pdf_urls):\n",
    "    if not p_u:\n",
    "        #If there is no pdf at all, we skip it.\n",
    "        continue\n",
    "    else:\n",
    "        #All this does is gets the PDF and downloads it and names it after the reg\n",
    "        #It looks scary and complicated but all it is doing is varying the name in the case of multiple PDFs\n",
    "        #Or naming it for the deceased person if there is no Ref value\n",
    "        #If there is a pdf but no ref or deceased name, this will throw an error and we can adjust.\n",
    "        try:\n",
    "            counter = 0\n",
    "            if len(p_u) > 1:\n",
    "                for p in p_u:\n",
    "                    if counter == 0:\n",
    "                        myfile = get(p)\n",
    "                        named = False\n",
    "                        for x in potential_names:\n",
    "                            try:\n",
    "                                if r_t[x]:\n",
    "                                    save_file(save_path, r_t[x])\n",
    "                                    counter +=1\n",
    "                                    named = True\n",
    "                                    break\n",
    "                                else:\n",
    "                                    continue\n",
    "                            except KeyError:\n",
    "                                continue\n",
    "                        if not named:       \n",
    "                            save_file(save_path, 'check_record_{}'.format(record_count))\n",
    "                            counter +=1\n",
    "\n",
    "                    else:\n",
    "                        myfile = get(p)\n",
    "                        named = False\n",
    "                        for x in potential_names:\n",
    "                            try:\n",
    "                                if r_t[x]:\n",
    "                                    save_file(save_path, r_t[x] + '_{}'.format(counter))\n",
    "                                    counter +=1\n",
    "                                    named = True\n",
    "                                    break\n",
    "                                else:\n",
    "                                    continue\n",
    "                            except KeyError:\n",
    "                                continue\n",
    "                        if not named:\n",
    "                            save_file(save_path, 'check_record_{}_{}'.format(record_count, counter))\n",
    "                            counter +=1\n",
    "                                    \n",
    "            else:\n",
    "                myfile = get(p_u[0])\n",
    "                named = False\n",
    "                for x in potential_names:\n",
    "                    try:\n",
    "                        if r_t[x]:\n",
    "                            save_file(save_path, r_t[x])\n",
    "                            named = True\n",
    "                            break\n",
    "                        else:\n",
    "                            continue\n",
    "                    except KeyError:\n",
    "                        continue\n",
    "                if not named:       \n",
    "                    save_file(save_path, 'check_record_{}'.format(record_count))\n",
    "            \n",
    "            record_count += 1\n",
    "        \n",
    "        except Exception as e:\n",
    "            import sys\n",
    "            if r_t['ref']:\n",
    "                raise type(e)(str(e) + '\\n' + 'Error for Record: {}'.format(r_t['ref'])).with_traceback(sys.exc_info()[2])\n",
    "            else:\n",
    "                raise type(e)(str(e) + '\\n' + 'Error for Record Number: {}'.format(record_count)).with_traceback(sys.exc_info()[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is my final step that puts the text data (info on the decreased/case) into a csv file & adds the date it was pulled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = \"Date of report: 19 JuneÂ  2014\"\n",
    "st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "headers = ['date_of_report', 'ref', 'deceased_name', 'coroner_name', 'coroner_area', 'category', \"this_report_is_being_sent_to\", \"url\"]\n",
    "\n",
    "with open('death_info_{}.csv'.format(date.today()), 'w', newline='', encoding='utf-8') as deaths_csv:\n",
    "    writer = csv.DictWriter(deaths_csv, fieldnames=headers)\n",
    "    writer.writeheader()\n",
    "    for record in record_text:\n",
    "        if record == {}:\n",
    "            pass\n",
    "        else:\n",
    "            writer.writerow(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an addition few steps to check what differences there are from the Dec month records "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "pdfs4 = os.listdir('All_PDFs4')\n",
    "pdfs5 = os.listdir('All_PDFs5')\n",
    "\n",
    "new_not_old = set(pdfs5).difference(pdfs4)\n",
    "\n",
    "new_not_old_list = list(new_not_old)\n",
    "new_not_old_list.sort()\n",
    "new_not_old_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_not_old_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feb = pd.read_csv('death_info_2020-02-05.csv')\n",
    "aug = pd.read_csv('death_info_2020-08-13.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(aug.columns)\n",
    "merged = aug.merge(feb, on=cols, how='left', indicator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_only = merged[merged['_merge'] == 'left_only']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(l_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_only.to_csv(r'death_info_newaug.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
