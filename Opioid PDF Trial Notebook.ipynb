{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from requests import ConnectionError\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from time import time\n",
    "import csv\n",
    "\n",
    "try:\n",
    "    get_ipython\n",
    "    from tqdm import tqdm_notebook as tqdm\n",
    "except NameError:\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "    \n",
    "def get_url(url):\n",
    "    response = get(url, verify = False)\n",
    "    html = response.content\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this uses my function to get the page with lots of names on it\n",
    "soup = get_url('https://www.judiciary.uk/subject/prevention-of-future-deaths/page/2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is an example of how to use find_all\n",
    "h5s = soup.find_all('h5', {'class': 'entry-title'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.judiciary.uk/publications/deborah-chapman/'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this is how you get a string of the URL in an h5\n",
    "h5s[0].a.get('href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is how you create a list of numbers that you will need for the loop\n",
    "pages = list(range(1,281))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is your first loop. This will go to each page in the 280 and pull out each individual record's url\n",
    "page_string = 'https://www.judiciary.uk/subject/prevention-of-future-deaths/page/{}/'\n",
    "record_urls = []\n",
    "for page in pages:\n",
    "    soup = get_url(page_string.format(str(page)))\n",
    "    h5s = soup.find_all('h5', {'class': 'entry-title'})\n",
    "    for h5 in h5s:\n",
    "        record_urls.append(h5.a.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is your second loop. It will loop through the list you just created and visit each individual record\n",
    "#you will then put inside the loop a way to pull out and store the text data and the pdf url\n",
    "#put these in two lists\n",
    "pdf_urls = []\n",
    "record_text =[]\n",
    "for record in record_urls:\n",
    "    #visit each page and pull out the info you need and the pdf url\n",
    "    #output here will be 2 lists, the pdf urls and your text data\n",
    "    #you want to pull out the text data in a way that is accessible\n",
    "    #and then loop through it with the pdf urls and have your code do:\n",
    "    #get this pdf, and call it the ref number\n",
    "    #Make the text data into a dictionarty where the things to the left of the colon are your keys (which are the same for every dict)\n",
    "    #and the things to the right are the changing values for each patient\n",
    "    #each thing you append to \"record_text\" will be a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your last loop will look something like this\n",
    "\n",
    "for pdf_link, p_d in zip(pdf_links, patient_data):\n",
    "    counter = 0\n",
    "    if len(pdf_link) > 1:\n",
    "        for pdf in pdf_link:\n",
    "            if counter == 0:\n",
    "                p = pdf.findNext('a').get('href')\n",
    "                myfile = get(pdf_link)\n",
    "                open('/deathpdfs/{}.pdf'.format(p_d['ref']), 'wb'.write(myfile.content))\n",
    "                counter +=1\n",
    "            else:\n",
    "                p = pdf.findNext('a').get('href')\n",
    "                myfile = get(pdf_link)\n",
    "                open('/deathpdfs/{}_{}.pdf'.format(ref, str(counter)), 'wb'.write(myfile.content))\n",
    "                counter +=1\n",
    "    else:\n",
    "        p = pdf_link[0].findNext('a').get('href')\n",
    "        myfile = get(pdf_link)\n",
    "        open('/deathpdfs/{}.pdf'.format(ref), 'wb'.write(myfile.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = get_url('https://www.judiciary.uk/publications/isaac-bahar/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date of report: 1 August 2019\n",
      "Ref: 2019-0280\n",
      "Deceased name: Deborah Chapman\n",
      "Coroners name: Adrian Farrow\n",
      "Coroners Area: Manchester (South)\n",
      "Category: Community health care related deaths; Alcohol, drug and medication related deaths\n",
      "This report is being sent to: West Timperley Medical Centre\n"
     ]
    }
   ],
   "source": [
    "death_info = soup.find('div', {'class':'entry-content'}).find_all('p')\n",
    "for p in death_info:\n",
    "    print(p.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_url = soup.find_all('li', {'class':'pdf'}).findNext('a').get('href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<li class=\"pdf\">\n",
       " <div class=\"pull-left\">\n",
       " <a href=\"https://www.judiciary.uk/wp-content/uploads/2015/10/Bahar-2015-0229.pdf\" title=\"Bahar 2015-0229\">Bahar 2015-0229</a>\n",
       " <ul class=\"list-inline\"><li><span class=\"info\">pdf</span></li>\n",
       "                     | <li><span class=\"title\">size: </span><span class=\"info\">0.57MB</span></li>\n",
       " </ul>\n",
       " </div>\n",
       " </li>, <li class=\"pdf\">\n",
       " <div class=\"pull-left\">\n",
       " <a href=\"https://www.judiciary.uk/wp-content/uploads/2015/06/2015-0229-Response-by-Brighton-and-Sussex-University-Hospitals-Trust.pdf\" title=\"2015-0229 Response by Brighton and Sussex University Hospitals Trust\">2015-0229 Response by Brighton and Sussex University Hospitals Trust</a>\n",
       " <ul class=\"list-inline\"><li><span class=\"info\">pdf</span></li>\n",
       "                     | <li><span class=\"title\">size: </span><span class=\"info\">0.27MB</span></li>\n",
       " </ul>\n",
       " </div>\n",
       " </li>]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#if you append this to a list in the loop you can write a sub loop that can account for multiple pdfs\n",
    "soup.find_all('li', {'class':'pdf'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "794353"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myfile = get(pdf_url)\n",
    "open(death_info[1].text.split(':')[1].strip() + '.pdf', 'wb').write(myfile.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2019-0280.pdf'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "death_info[1].text.split(':')[1].strip() + '.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Date of report', ' 1 August 2019']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thing = 'Date of report: 1 August 2019'\n",
    "thing.split(':')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "bb\n",
      "ccc\n"
     ]
    }
   ],
   "source": [
    "l1 = [pdf_url_1, 2, 3]\n",
    "l2 = ['ref 1', 'ref 2', 'c']\n",
    "\n",
    "for num, let in zip(l1, l2):\n",
    "    print(let * num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = ['date_of_repot', 'ref','']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
