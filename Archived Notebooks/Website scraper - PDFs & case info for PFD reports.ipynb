{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping the Courts and Tribunals Judiciary Website for Prevent Future Death Reports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from requests import ConnectionError\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from time import sleep\n",
    "from time import time\n",
    "import csv\n",
    "\n",
    "try:\n",
    "    get_ipython\n",
    "    from tqdm import tqdm_notebook as tqdm\n",
    "except NameError:\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "    \n",
    "def get_url(url):\n",
    "    response = get(url, verify = False)\n",
    "    html = response.content\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraper starts here - this was run on Monday, Dec 2 2019 at 12.00 pm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your second value in \"range\" will be one more than the number of pages that exist on the wesbite\n",
    "pages = list(range(1,288))\n",
    "\n",
    "#This loops through all the pages to get the URLs to individual records\n",
    "page_string = 'https://www.judiciary.uk/subject/prevention-of-future-deaths/page/{}/'\n",
    "record_urls = []\n",
    "for page in tqdm(pages):\n",
    "    soup = get_url(page_string.format(str(page)))\n",
    "    h5s = soup.find_all('h5', {'class': 'entry-title'})\n",
    "    for h5 in h5s:\n",
    "        record_urls.append(h5.a.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we check how many records (i.e. cases) were pulled from the urls & the first and last case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(record_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_urls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_urls[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is my second loop. This will go through the lists of URLs I just created above to visit each individual record and pull out and store the text data (info on the decreased/case) and the PDF URL I will use later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_exp = re.compile(r\"â€™s\\s|s\\s|'s\\s\")\n",
    "text_cats = ['Date of report', 'Ref', 'Deceased name', 'Coroner name', 'Coroner Area', 'Category', \"This report is being sent to\"]\n",
    "#First, I create two lists, one for the PDFs and one for the text data\n",
    "record_text = []\n",
    "pdf_urls = []\n",
    "#I want to loop through each URL & pull out the death information and pdf link for downloading \n",
    "for record_url in tqdm(record_urls):\n",
    "    #This is just a way to retry reaching the website incase there is a temporary error in reaching a specific page\n",
    "    #If it fails more than 3 times, then you will get an error, but this protects against a temporary blip ruining the scrape\n",
    "    try:\n",
    "        tries = 3\n",
    "        for i in range(tries):\n",
    "            try:\n",
    "                soup = get_url(record_url)\n",
    "            except (ConnectionError, SSLError):\n",
    "                if i < tries - 1:\n",
    "                    sleep(2)\n",
    "                    continue\n",
    "                else:\n",
    "                    raise\n",
    "        #This gets all the text fields from the website to work with\n",
    "        death_info = soup.find('div', {'class':'entry-content'}).find_all('p')\n",
    "        #Our dictionary that will hold all of the text information that we will eventually append to \"record_text\"\n",
    "        blankdict = {}\n",
    "        #This is to handle 1 annoying record with messed up html tags\n",
    "        if record_url == 'https://www.judiciary.uk/publications/roadsafety/':\n",
    "            strong = death_info[0].find_all('strong')\n",
    "            heads = ['date_of_report', 'ref', 'deceased_name', 'coroner_name', 'coroner_area', 'category']\n",
    "            for st, h in zip(strong,heads):\n",
    "                blankdict[h] = st.next_sibling.replace(':',\"\").strip()\n",
    "        #looping through all of the text categories for handling\n",
    "        for p in death_info:\n",
    "            #This checks for blank fields and if there is nothing, it skips it\n",
    "            if p.text.strip() == '':\n",
    "                pass\n",
    "            #This checks for our \"Normal\" case in which a colon exists and the category is one of the ones we pre-specified\n",
    "            #above in the \"text_cats\" list\n",
    "            #We also need to account here for one strange record for \"Rebecca Evans\" which has a weird text error\n",
    "            #That we manually correct for\n",
    "            elif ':' in p.text and p.text.split(':')[0] in text_cats and not 'Rebecca-EvansR.pdf' in p.text:\n",
    "                #Simply assigning the key and value from strings on either side of the colon, making everything lower case\n",
    "                #and replacing spaces with underscores and also removing any stray semi-colons\n",
    "                text_list = p.text.split(':')\n",
    "                blankdict[text_list[0].strip().replace(' ','_').lower()] = text_list[1].strip().replace('\\n','')\n",
    "            elif 'Rebecca-EvansR.pdf' in p.text:\n",
    "                #This deals with that singular odd record that currently exists as of 8 Nov 2019\n",
    "                blankdict['category'] = p.text.split(':')[1].strip().replace('\\n','')\n",
    "            elif ':' not in p.text:\n",
    "                #If the string don't have a colon, we can't split on it so have to get it into dictionary format\n",
    "                #Using an alternate method that counts the length of the thing\n",
    "                \n",
    "                if any(x in p.text for x in text_cats):\n",
    "                    t = [x for x in text_cats if x in p.text][0]\n",
    "                    l = len(t)\n",
    "                    blankdict[t.replace(' ','_').lower()] = p.text[l+1:].replace('\\n','')\n",
    "                elif 'Coroners Area' in p.text:\n",
    "                    blankdict['coroner_area'] = p.text[13:].strip().replace('\\n','')\n",
    "                else:\n",
    "                    print(\"Something we haven't accounted for has happened\")\n",
    "            \n",
    "            elif p.text.strip().count(\":\") == 2:\n",
    "                #This corrects for one odd record in which there are 2 colons but should generalize to fix it for\n",
    "                #any time this could happen, so long as it happens in the same way\n",
    "                text_list = p.text.split(':')\n",
    "                new_string = text_list[0] + text_list[1]\n",
    "                new_name = re.sub(reg_exp, ' ', new_string).strip()\n",
    "                blankdict[new_name.replace(' ','_').lower()] = text_list[2].strip().replace('\\n','')\n",
    "            \n",
    "            elif ':' in p.text and p.text.split(':')[0] not in text_cats:\n",
    "                #Some field names are in the form of \"name_of_decesased\" or \"name_of_coroner\" or are plural/possessive\n",
    "                #so this smashes those into our preferred naming formats\n",
    "                if 'Name of' in p.text:\n",
    "                    all_text = p.text.split(':')\n",
    "                    key_name = all_text[0].split(' ')\n",
    "                    blankdict[key_name[2].strip() + '_name'] = all_text[-1].strip()\n",
    "                else:    \n",
    "                    new_name = re.sub(reg_exp, ' ', p.text)\n",
    "                    text_list = new_name.split(':')\n",
    "                    blankdict[text_list[0].strip().replace(' ','_').lower()] = text_list[1].strip().replace('\\n','')\n",
    "        blankdict['url'] = record_url\n",
    "        #This appends the final dict to the list\n",
    "        record_text.append(blankdict)\n",
    "        #this is a seperate process to get the PDF URLs (no matter how many there are) and adds them to their own list   \n",
    "        urls = soup.find_all('li', {'class':'pdf'})\n",
    "        pdf_list = []\n",
    "        for url in urls:\n",
    "            pdf_list.append(url.findNext('a').get('href'))\n",
    "        pdf_urls.append(pdf_list)\n",
    "    except Exception as e:\n",
    "        #This is an exception catcher to give useful feedback for debugging\n",
    "        import sys\n",
    "        raise type(e)(str(e) + '\\n' + 'Error for Record: {}, Field: {}'.format(record_url, p)).with_traceback(sys.exc_info()[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the third loop to save the PDFs using the deceased Ref as the file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the final scrape to actually get the URLs and change the name (when possible) to the refs\n",
    "for r_t, p_u in zip(tqdm(record_text), pdf_urls):\n",
    "    if len(p_u) == 0:\n",
    "        #If there is no pdf at all, we skip it.\n",
    "        pass\n",
    "    else:\n",
    "        #All this does is gets the PDF and downloads it and names it after the reg\n",
    "        #It looks scary and complicated but all it is doing is varying the name in the case of multiple PDFs\n",
    "        #Or naming it for the deceased person if there is no Ref value\n",
    "        #If there is a pdf but no ref or deceased name, this will throw an error and we can adjust.\n",
    "        try:\n",
    "            counter = 0\n",
    "            if len(p_u) > 1:\n",
    "                for p in p_u:\n",
    "                    if counter == 0:\n",
    "                        myfile = get(p)\n",
    "                        if r_t['ref']:\n",
    "                            with open('/Users/georgiarichards/Desktop/Python stuff/PFDs opioids/All_PDFs2/{}.pdf'.format(r_t['ref']), 'wb') as d:\n",
    "                                d.write(myfile.content)\n",
    "                            counter +=1\n",
    "                        else:\n",
    "                            with open('/Users/georgiarichards/Desktop/Python stuff/PFDs opioids/All_PDFs2/{}.pdf'.format(r_t['deceased_name']), 'wb') as d:\n",
    "                                d.write(myfile.content)\n",
    "                            counter +=1\n",
    "                    else:\n",
    "                        myfile = get(p)\n",
    "                        if r_t['ref']:\n",
    "                            with open('/Users/georgiarichards/Desktop/Python stuff/PFDs opioids/All_PDFs2/{}_{}.pdf'.format(r_t['ref'], str(counter)), 'wb') as d:\n",
    "                                d.write(myfile.content)\n",
    "                            counter +=1\n",
    "                        else:\n",
    "                            with open('/Users/georgiarichards/Desktop/Python stuff/PFDs opioids/All_PDFs2/{}_{}.pdf'.format(r_t['deceased_name'], str(counter)), 'wb') as d:\n",
    "                                d.write(myfile.content)\n",
    "                            counter +=1\n",
    "            else:\n",
    "                myfile = get(p_u[0])\n",
    "                if r_t['ref']:\n",
    "                    with open('/Users/georgiarichards/Desktop/Python stuff/PFDs opioids/All_PDFs2/{}.pdf'.format(r_t['ref']), 'wb') as d:\n",
    "                        d.write(myfile.content)\n",
    "                else:\n",
    "                    with open('/Users/georgiarichards/Desktop/Python stuff/PFDs opioids/All_PDFs2/{}.pdf'.format(r_t['deceased_name']), 'wb') as d:\n",
    "                        d.write(myfile.content)\n",
    "        except Exception as e:\n",
    "            import sys\n",
    "            if r_t['ref']:\n",
    "                raise type(e)(str(e) + '\\n' + 'Error for Record: {}'.format(r_t['ref'])).with_traceback(sys.exc_info()[2])\n",
    "            else:\n",
    "                raise type(e)(str(e) + '\\n' + 'Error for Record: {}'.format(r_t['deceased_name'])).with_traceback(sys.exc_info()[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is my final step that puts the text data (info on the decreased/case) into a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "headers = ['date_of_report', 'ref', 'deceased_name', 'coroner_name', 'coroner_area', 'category', \"this_report_is_being_sent_to\", \"url\"]\n",
    "\n",
    "\n",
    "with open('death_info_{}.csv'.format(date.today()), 'w', newline='', encoding='utf-8') as deaths_csv:\n",
    "    writer = csv.DictWriter(deaths_csv, fieldnames=headers)\n",
    "    writer.writeheader()\n",
    "    for record in record_text:\n",
    "        if record == {}:\n",
    "            pass\n",
    "        else:\n",
    "            writer.writerow(record)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
